{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Pneumonia X-ray image analysis\n",
    "\n",
    "**Neural Networks for Machine Learning Applications**\\\n",
    "Rabindra Manandhar\n",
    "\n",
    "14.2.2024 - version 1\\\n",
    "28.2.2024 - version 2\n",
    "\n",
    "Information Technology, Bachelor's Degree\n",
    "Metropolia University of Applied Sciences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook creates a binary classifier for x-ray chest images using convolutional neural networks. The main objective is to detect and classify human diseases from the medical images.\n",
    "\n",
    "The task is to develop a minimum of three different CNN models, calculate the classification reports and confusion matrices for the outcomes and compare their results. The aim is to achieve a minimum of 90% of sensitivity and 90% of specificity in classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this case is obtained from the `https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia/data`.\n",
    "\n",
    "The dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia/Normal).\n",
    "\n",
    "train => contains the training data/images for teaching our model.\n",
    "\n",
    "val => contains images which we will use to validate our model. The purpose of this data set is to prevent our model from Overfitting. Overfitting is when your model gets a little too comofortable with the training data and can't handle data it hasn't seen too well.\n",
    "\n",
    "As the original dataset has only 16 images in the validation folder, the validation dataset is obtained by separating the training dataset into training set/validation set at 80/20. \n",
    "\n",
    "test => this contains the data that we use to test the model once it has learned the relationships between the images and their label (Pneumonia/Not-Pneumonia)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WwvTTRihbCA-",
    "ExecuteTime": {
     "end_time": "2024-11-28T12:35:04.850177Z",
     "start_time": "2024-11-28T12:35:04.844647Z"
    }
   },
   "source": [
    "import os, shutil\n",
    "\n",
    "# In the following, insert appropriate path for where the original data is located on your file system,and also the path\n",
    "# to the destination folder (which is created when executing this).\n",
    "\n",
    "# Define the path to the original and base directories\n",
    "original_dir = \"/Users/rabindramanandhar/Documents/Metropolia/2024/NeuralNetworksForMachineLearning/Deep-Learning/case2/chest_xray\"\n",
    "base_dir = \"/Users/rabindramanandhar/Documents/Metropolia/2024/NeuralNetworksForMachineLearning/Deep-Learning/case2/base\"\n",
    "\n",
    "# Create base directory\n",
    "os.makedirs(base_dir, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "69-CbQPlbCA_",
    "ExecuteTime": {
     "end_time": "2024-11-28T12:35:04.857303Z",
     "start_time": "2024-11-28T12:35:04.854640Z"
    }
   },
   "source": [
    "# Define the paths to train, test and validation subdirectories\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "\n",
    "# Create the train, test and validation subdirectories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(validation_dir, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_DYLtw5jbCBA",
    "ExecuteTime": {
     "end_time": "2024-11-28T12:35:04.874687Z",
     "start_time": "2024-11-28T12:35:04.868420Z"
    }
   },
   "source": [
    "# Define the paths to normal and pneumonia subdirectories for train, test and validation\n",
    "\n",
    "train_normal_dir = os.path.join(train_dir, \"NORMAL\")\n",
    "train_pneumonia_dir = os.path.join(train_dir, \"PNEUMONIA\")\n",
    "\n",
    "test_normal_dir = os.path.join(test_dir, \"NORMAL\")\n",
    "test_pneumonia_dir = os.path.join(test_dir, \"PNEUMONIA\")\n",
    "\n",
    "validation_normal_dir = os.path.join(validation_dir, \"NORMAL\")\n",
    "validation_pneumonia_dir = os.path.join(validation_dir, \"PNEUMONIA\")\n",
    "\n",
    "# Create the normal and pneumonia subdirectories for train, test and validation.\n",
    "\n",
    "os.makedirs(train_normal_dir, exist_ok=True)\n",
    "os.makedirs(train_pneumonia_dir, exist_ok=True)\n",
    "os.makedirs(test_normal_dir, exist_ok=True)\n",
    "os.makedirs(test_pneumonia_dir, exist_ok=True)\n",
    "os.makedirs(validation_normal_dir, exist_ok=True)\n",
    "os.makedirs(validation_pneumonia_dir, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oT9axSoabCBB",
    "outputId": "fa79de7c-d7a8-48ef-ebc9-5844ff3d72f8",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-28T12:35:04.890561Z",
     "start_time": "2024-11-28T12:35:04.881333Z"
    }
   },
   "source": [
    "# Create lists with training file names\n",
    "\n",
    "# Define paths to original train and test directories\n",
    "original_train_dir = os.path.join(original_dir, \"train\")\n",
    "original_test_dir = os.path.join(original_dir, \"test\")\n",
    "\n",
    "original_train_normal_dir = os.path.join(original_train_dir, \"NORMAL\")\n",
    "original_train_pneumonia_dir = os.path.join(original_train_dir, \"PNEUMONIA\")\n",
    "\n",
    "original_test_normal_dir = os.path.join(original_test_dir, \"NORMAL\")\n",
    "original_test_pneumonia_dir = os.path.join(original_test_dir, \"PNEUMONIA\")\n",
    "\n",
    "# List files in the normal and pneumonia subdirectories of the original train directory\n",
    "train_names_normal = os.listdir(original_train_normal_dir)\n",
    "train_names_pneumonia = os.listdir(original_train_pneumonia_dir)\n",
    "\n",
    "# List files in the normal and pneumonia subdirectories of the original test directory\n",
    "test_names_normal = os.listdir(original_test_normal_dir)\n",
    "test_names_pneumonia = os.listdir(original_test_pneumonia_dir)\n",
    "\n",
    "print(\"Train directory:\")\n",
    "print(len(train_names_normal), \"normal images\")\n",
    "print(len(train_names_pneumonia), \"pneumonia images\")\n",
    "\n",
    "print(\"Test directory:\")\n",
    "print(len(test_names_normal), \"normal images\")\n",
    "print(len(test_names_pneumonia), \"pneumonia images\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train directory:\n",
      "1349 normal images\n",
      "3884 pneumonia images\n",
      "Test directory:\n",
      "234 normal images\n",
      "390 pneumonia images\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wrlgAg-3bCBC",
    "ExecuteTime": {
     "end_time": "2024-11-28T12:35:04.933549Z",
     "start_time": "2024-11-28T12:35:04.920367Z"
    }
   },
   "source": [
    "# Shuffle the lists in random order\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(train_names_normal)\n",
    "random.shuffle(train_names_pneumonia)\n",
    "random.shuffle(test_names_normal)\n",
    "random.shuffle(test_names_pneumonia)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2eLSDEDnbCBD",
    "outputId": "166d7b14-14b1-4988-cc20-9c6657f9b692",
    "ExecuteTime": {
     "end_time": "2024-11-28T12:35:04.963567Z",
     "start_time": "2024-11-28T12:35:04.961381Z"
    }
   },
   "source": [
    "# Compute splitting points for train/validation/test with 80%/20% to be used below\n",
    "\n",
    "print(\"Train data normal:\")\n",
    "print(\"80%\", int(0.8 * len(train_names_normal)))\n",
    "\n",
    "print(\"Train data pneumonia:\")\n",
    "print(\"80%\", int(0.8 * len(train_names_pneumonia)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data normal:\n",
      "80% 1079\n",
      "Train data pneumonia:\n",
      "80% 3107\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T12:35:08.785794Z",
     "start_time": "2024-11-28T12:35:04.983160Z"
    }
   },
   "source": [
    "# Copy original training data to new destination base directories in its own subdirectories 80% to train and 20% to validation\n",
    "\n",
    "# from chest_xray > train > normal to base > train > normal subdirectory\n",
    "for fname in train_names_normal[:1072]:\n",
    "    src = os.path.join(original_train_normal_dir, fname)\n",
    "    dst = os.path.join(train_normal_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# from chest_xray > train > normal to base > validation > normal subdirectory\n",
    "for fname in train_names_normal[1072:]:\n",
    "    src = os.path.join(original_train_normal_dir, fname)\n",
    "    dst = os.path.join(validation_normal_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# from chest_xray > train > pneumonia to base > train > pneumonia subdirectory\n",
    "for fname in train_names_pneumonia[:3100]:\n",
    "    src = os.path.join(original_train_pneumonia_dir, fname)\n",
    "    dst = os.path.join(train_pneumonia_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# from chest_xray > train > pneumonia to base > validation > pneumonia subdirectory\n",
    "for fname in train_names_pneumonia[3100:]:\n",
    "    src = os.path.join(original_train_pneumonia_dir, fname)\n",
    "    dst = os.path.join(validation_pneumonia_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy original test data to new destinatino base directories in its own subdirectories\n",
    "\n",
    "# from chest_xray > test > normal to base > test > normal subdirectory\n",
    "for fname in test_names_normal:\n",
    "    src = os.path.join(original_test_normal_dir, fname)\n",
    "    dst = os.path.join(test_normal_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# from chest_xray > test > pneumonia to base > test > pneumonia subdirectory\n",
    "for fname in test_names_pneumonia:\n",
    "    src = os.path.join(original_test_pneumonia_dir, fname)\n",
    "    dst = os.path.join(test_pneumonia_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3NRa7WJKbCBH",
    "outputId": "41f6dda0-be09-4c06-ee03-91e1ecaefdc5",
    "ExecuteTime": {
     "end_time": "2024-11-28T12:35:08.839188Z",
     "start_time": "2024-11-28T12:35:08.810449Z"
    }
   },
   "source": [
    "# Check the contents of the new directories\n",
    "\n",
    "print(\"Total normal training samples:\", len(os.listdir(train_normal_dir)))\n",
    "print(\"Total normal validation samples:\", len(os.listdir(validation_normal_dir)))\n",
    "print(\"Total normal test samples:\", len(os.listdir(test_normal_dir)))\n",
    "\n",
    "print(\"Total pneumonia training samples:\", len(os.listdir(train_pneumonia_dir)))\n",
    "print(\"Total pneumonia validation samples:\", len(os.listdir(validation_pneumonia_dir)))\n",
    "print(\"Total pneumonia test samples:\", len(os.listdir(test_pneumonia_dir)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total normal training samples: 1349\n",
      "Total normal validation samples: 1179\n",
      "Total normal test samples: 234\n",
      "Total pneumonia training samples: 3884\n",
      "Total pneumonia validation samples: 3371\n",
      "Total pneumonia test samples: 390\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required libraries are as follows:\n",
    "\n",
    "os - os module allows to access os functionalities and interact with the underlying os that Python is running on.\n",
    "\n",
    "shutil - for copying files and entire directory trees, as well as for archiving files and directories.\n",
    "\n",
    "random - to shuffle the images\n",
    "\n",
    "tensorflow - for building and training neural networks  of various architectures ( CNNs in this case)\n",
    "\n",
    "matplotlib, seaborn - to visualize/make graphical presentations of the training results\n",
    "\n",
    "scikit-learn - it provides a wide range of tools for building machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T12:35:08.852049Z",
     "start_time": "2024-11-28T12:35:08.846194Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"NumPy version:\", np.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "NumPy version: 2.0.2\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing / Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid overfitting problem, we need to expand artificially our dataset. Data augmentation takes the approach of generating more training data from existing training samples by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that, at training time, the model will never see the exact same picture twice. This helps expose the model to more aspects of the data so it can generalize better.\n",
    "\n",
    "In Keras, this can be done by adding a number of data augmentation layers at the start of the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T12:35:08.870218Z",
     "start_time": "2024-11-28T12:35:08.868285Z"
    }
   },
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the flow_from_directory() method from the Keras ImageDataGenerator class to create a data generator for training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T12:35:08.964396Z",
     "start_time": "2024-11-28T12:35:08.875467Z"
    }
   },
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  # This is the sub-directory in base directory for training images from\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir, target_size=(150, 150), batch_size=32, class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir, target_size=(150, 150), batch_size=32, class_mode=\"binary\", shuffle=False\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5232 images belonging to 2 classes.\n",
      "Found 4549 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Keras with TensorFlow, we build a CNN model with convolutional neural network (CNN) architecture. This architecture consists of convolutional layers followed by max-pooling layers to extract features from the input images. The 'same padding' applies padding to the input images so that the input image gets fully covered by the filter and specified stride. For stride 1, with same padding, the output will be the same as the input. The flattened output is then passed through fully connected layers with dropout regularization to perform classification. Finally, the sigmoid activation function in the output layer provides the probability of the input image belonging to the positive class in a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T12:35:09.067604Z",
     "start_time": "2024-11-28T12:35:08.972590Z"
    }
   },
   "source": [
    "# Architecture\n",
    "model = Sequential(\n",
    "    [\n",
    "        Conv2D(\n",
    "            32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(150, 150, 3)\n",
    "        ),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        # BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        # BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        # BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dropout(0.4),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the model ready for training, we need to pick three more things as part of the compilation step:\n",
    "\n",
    "An optimizer — The mechanism through which the model will update the weights of the neural network based on the training data it sees during training , so as to improve its performance i.e. to minimize the loss function.\n",
    "\n",
    "A loss function — How the model will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.\n",
    "\n",
    "Metrics - to monitor and evaluate the performance of the model during training and testing — Here, we’ll only care about accuracy (the fraction of the images that were correctly classified)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T12:39:18.959583Z",
     "start_time": "2024-11-28T12:39:18.881870Z"
    }
   },
   "source": [
    "# Compilation\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(),\n",
    "    loss=losses.BinaryCrossentropy(),\n",
    "    metrics=[tf.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m150\u001B[0m, \u001B[38;5;34m150\u001B[0m, \u001B[38;5;34m32\u001B[0m)   │           \u001B[38;5;34m896\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m75\u001B[0m, \u001B[38;5;34m75\u001B[0m, \u001B[38;5;34m32\u001B[0m)     │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m75\u001B[0m, \u001B[38;5;34m75\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │        \u001B[38;5;34m18,496\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m37\u001B[0m, \u001B[38;5;34m37\u001B[0m, \u001B[38;5;34m64\u001B[0m)     │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m37\u001B[0m, \u001B[38;5;34m37\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │        \u001B[38;5;34m73,856\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m18\u001B[0m, \u001B[38;5;34m18\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001B[38;5;33mConv2D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m18\u001B[0m, \u001B[38;5;34m18\u001B[0m, \u001B[38;5;34m128\u001B[0m)    │       \u001B[38;5;34m147,584\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (\u001B[38;5;33mMaxPooling2D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m9\u001B[0m, \u001B[38;5;34m128\u001B[0m)      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001B[38;5;33mFlatten\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10368\u001B[0m)          │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)            │     \u001B[38;5;34m5,308,928\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m512\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │           \u001B[38;5;34m513\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10368</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,308,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m5,550,273\u001B[0m (21.17 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,550,273</span> (21.17 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m5,550,273\u001B[0m (21.17 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,550,273</span> (21.17 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In keras, fit() method is used to train the neural network model on a given dataset.\n",
    "\n",
    "During training, the model will perform the following steps for each epoch:\n",
    "\n",
    "Forward pass: The model takes the input data (train_generator) and passes it through the neural network to generate predictions.\n",
    "\n",
    "Computation of loss: The model computes the loss, which is a measure of how well the predictions match the true labels (train_generator.labels).\n",
    "\n",
    "Backward pass (Backpropagation): The model calculates the gradients of the loss with respect to the model's parameters (weights and biases) using backpropagation.\n",
    "\n",
    "Optimization: The optimizer adjusts the model's parameters based on the computed gradients to minimize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 10 # Increase the number of epochs for better convergence\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "\n",
    "- Show the training and validation loss and accuracy plots\n",
    "- Interpret the loss and accuracy plots (e.g. is there under- or over-fitting)\n",
    "- Describe the final performance of the model with test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model, we check that on average, how good is our model at classifying never-before-seen digits by computing average accuracy over the entire test set using evaluate() method in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what data we have in the h -structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dict = history.history\n",
    "h_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the training and validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "\n",
    "    plt.plot(history.history[\"binary_accuracy\"], label=\"train\")\n",
    "    plt.plot(history.history[\"val_binary_accuracy\"], label=\"validation\")\n",
    "    # plt.ylim([0, 10])\n",
    "    plt.title(\"Training accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy(history)\n",
    "plt.ylim(0.5, 1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "\n",
    "    plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"validation\")\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plt.ylim(-0.05, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and demonstration of metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the classification report and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the test dataset for evaluation\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "\n",
    "print(f\"Classification Report:\")\n",
    "\n",
    "# Get predicted probabilities\n",
    "test_pred_prob = model.predict(test_generator)\n",
    "\n",
    "# Convert probabilities to binary predictions based on a threshold (e.g., 0.5)\n",
    "test_pred = (test_pred_prob > 0.5).astype(int)\n",
    "test_labels = test_generator.labels\n",
    "\n",
    "class_names = [\"NORMAL\", \"PNEUMONIA\"]\n",
    "report = classification_report(test_labels, test_pred, target_names=class_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "`Precision`: Precision measures the proportion of true positive predictions out of all positive predictions made by the model. It is computed as PPV = TP / (TP + FP)\n",
    "\n",
    "For class NORMAL, the precision is *, indicating that *% of the predictions for class NORMAL are correct.\\\n",
    "For class PNEUMONIA, the precision is *, meaning only *% of the predictions for class PNEUMONIA are correct.\\\n",
    "\n",
    "`Recall`: Recall (also known as `sensitivity`) measures the rate of true positive cases correctly predicted. It is computed as TPR = TP / (TP + FN).\n",
    "\n",
    "For class NORMAL, the recall is *, indicating that *% of the actual instances of class NORMAL were correctly predicted by the model.\\\n",
    "For class PNEUMONIA, the recall is *, meaning *% of the actual instances of class PNEUMONIA were correctly predicted by the model.\\\n",
    "F1-score: The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall.\n",
    "\n",
    "For class NORMAL, the F1-score is *, indicating a decent balance between precision and recall for class NORMAL.\\\n",
    "For class PNEUMONIA, the F1-score is *, suggesting that the model struggles to balance precision and recall for class PNEUMONIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "print(f\"Confusion Matrix:\")\n",
    "\n",
    "conf_matrix = confusion_matrix(test_labels, test_pred)\n",
    "# print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(conf_matrix)\n",
    "disp.plot()\n",
    "print(confusion_matrix)\n",
    "print(\"True Positives: \", conf_matrix[1, 1])\n",
    "print(\"False Negatives: \", conf_matrix[0, 1])\n",
    "print(\"False positives: \", conf_matrix[1, 0])\n",
    "print(\"True Negatives: \", conf_matrix[0, 0])\n",
    "\n",
    "# Compute sensitivity, specificity and overall accuracy\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"Overall Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A confusion matrix is a table used in classification tasks to evaluate the performance of a machine learning model. It presents a summary of the predictions made by the model against the actual labels in a tabular format. Each cell in the matrix represents the count of instances where the model predicted a certain class compared to the true class.\n",
    "\n",
    "**True Positives (TP):** The model correctly predicted * instances as positive (e.g., correctly identified positive cases).\n",
    "\n",
    "**False Positives (FP):** The model incorrectly predicted * instances as positive (e.g., falsely identified negative cases as positive).\n",
    "\n",
    "**True Negatives (TN):** The model correctly predicted * instances as negative (e.g., correctly identified negative cases).\n",
    "\n",
    "**False Negatives (FN):** The model incorrectly predicted * instances as negative (e.g., falsely identified positive cases as negative).\n",
    "\n",
    "The above confusion matrix suggests that the model has:\n",
    "\n",
    "High false positives and low false negatives, indicating that the model is prone to making Type I errors (false alarms).\n",
    "\n",
    "High true negatives and low true positives, which implies that the model may have difficulty identifying positive instances accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(test_labels, test_pred_prob)\n",
    "auc = roc_auc_score(test_labels, test_pred_prob)\n",
    "plt.plot(fpr, tpr, label=\"Model (auc = {:.3f})\".format(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.xlim(\n",
    "    0,\n",
    ")\n",
    "plt.ylim(\n",
    "    0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Discussion and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Write\n",
    "\n",
    "- What settings and models were tested before the best model was found\n",
    "    - What where the results of these experiments\n",
    "- Summary of  \n",
    "    - What was your best model and its settings\n",
    "    - What was the final achieved performance\n",
    "- What are your main observations and learning points\n",
    "- Discussion how the model could be improved in future\n",
    "\n",
    "**Note:** Remember to evaluate the final metrics using the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we have used CNN's sequential modeling architecture. We used Conv2D with several filters (32, 64, 128) of filter sizes 3x3 followed by Maxpooling2D layer of pool size 2x2. The flattened output is then passed through fully connected layers with dropout regularization to perform classification. Finally, the sigmoid activation function in the output layer provides the probability of the input image belonging to the positive class in a binary classification problem.\n",
    "\n",
    "The **adam** optimizer with default settings was used.\n",
    "\n",
    "Totally ~98% of sensitivity ~72% of specificity for the test dataset were achieved with 10 epochs.\n",
    "\n",
    "The model might be tuned by using more epochs to train the model and then the desired specificity might also be achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
